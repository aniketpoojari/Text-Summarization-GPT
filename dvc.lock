schema: '2.0'
stages:
  create_pretraining_data:
    cmd: python src/create_pretraining_data.py --config=params.yaml
    deps:
    - path: src/create_pretraining_data.py
      hash: md5
      md5: 46854de467615cff9329b59e0761bbfa
      size: 704
    params:
      params.yaml:
        data.pretraining_parquet: data/raw/pretraining/train-00001-of-00002.parquet
        data.pretraining_txt: data/raw/pretraining/pretraining.txt
    outs:
    - path: data/raw/pretraining/pretraining.txt
      hash: md5
      md5: 2938ccdd6d68ddb37845a262020aa61f
      size: 270479474
  create_tokenizer:
    cmd: python src/create_tokenizer.py --config=params.yaml
    deps:
    - path: data/raw/pretraining/pretraining.txt
      hash: md5
      md5: 2938ccdd6d68ddb37845a262020aa61f
      size: 270479474
    - path: src/create_tokenizer.py
      hash: md5
      md5: bf0af90be37de8d28f541c051108a21d
      size: 660
    params:
      params.yaml:
        data.pretraining_txt: data/raw/pretraining/pretraining.txt
        data.vocab_dir: data/tokenizer/spm_model
        hyperparameters.vocab_size: 10000
    outs:
    - path: data/tokenizer/spm_model.model
      hash: md5
      md5: 7af9c385fcb3ced1bbe55c6b15890f33
      size: 409670
  create_pretraining_split:
    cmd: python src/create_pretraining_split.py --config=params.yaml
    deps:
    - path: data/raw/pretraining/pretraining.txt
      hash: md5
      md5: 2938ccdd6d68ddb37845a262020aa61f
      size: 270479474
    - path: data/tokenizer/spm_model.model
      hash: md5
      md5: 7af9c385fcb3ced1bbe55c6b15890f33
      size: 409670
    - path: src/create_pretraining_split.py
      hash: md5
      md5: d5af4250321f5b62c7e52f981f225520
      size: 1323
    params:
      params.yaml:
        data.pretraining_train: data/raw/pretraining/pretraining_train.pkl
        data.pretraining_txt: data/raw/pretraining/pretraining.txt
        data.pretraining_val: data/raw/pretraining/pretraining_val.pkl
        data.vocab_dir: data/tokenizer/spm_model
    outs:
    - path: data/raw/pretraining/pretraining_train.pkl
      hash: md5
      md5: 0070b22d1b69f11e4ad73fd0355c96e3
      size: 226282912
    - path: data/raw/pretraining/pretraining_val.pkl
      hash: md5
      md5: 520534386937c3074e1fbe05f41ec361
      size: 24426380
  training_pretraining:
    cmd: python src/training.py --config=params.yaml
    deps:
    - path: data/raw/pretraining/pretraining_train.pkl
      hash: md5
      md5: 0070b22d1b69f11e4ad73fd0355c96e3
      size: 226282912
    - path: data/raw/pretraining/pretraining_val.pkl
      hash: md5
      md5: 520534386937c3074e1fbe05f41ec361
      size: 24426380
    - path: src/training.py
      hash: md5
      md5: 856627f5e8d922507eaf7f7bfda51784
      size: 4663
    params:
      params.yaml:
        data.pretraining_train: data/raw/pretraining/pretraining_train.pkl
        data.pretraining_val: data/raw/pretraining/pretraining_val.pkl
        hyperparameters.batch_size: 32
        hyperparameters.block_size: 128
        hyperparameters.dropout: 0.2
        hyperparameters.eval_interval: 100
        hyperparameters.eval_iters: 50
        hyperparameters.learning_rate: 0.0001
        hyperparameters.max_iters: 100
        hyperparameters.n_embd: 512
        hyperparameters.n_head: 8
        hyperparameters.n_layer: 8
        hyperparameters.vocab_size: 10000
        mlflow.experiment_name: Text Summarization
        mlflow.registered_model_name: GPT
        mlflow.run_name: 1st
        mlflow.server_uri: sqlite:///mlflow.db
    outs:
    - path: training_completion.txt
      hash: md5
      md5: 8de729e47e95597f6e3bbd8b3715b090
      size: 41
