data:
  pretraining_parquet: data/raw/pretraining/train-00001-of-00002.parquet
  pretraining_txt: data/raw/pretraining/pretraining.txt
  pretraining_train: data/raw/pretraining/pretraining_train.pkl
  pretraining_val: data/raw/pretraining/pretraining_val.pkl
  vocab_dir: data/tokenizer/spm_model

hyperparameters:
  vocab_size: 10000
  n_embd: 512
  block_size: 128
  n_head: 8
  n_layer: 8
  dropout: 0.2
  batch_size: 32
  learning_rate: 1e-4
  max_iters: 100
  eval_iters: 50
  eval_interval: 100

mlflow_pretraining:
  server_uri: sqlite:///mlflow.db
  experiment_name: Pretraining
  run_name: 1st
  registered_model_name: Pretrained GPT
