data:
  pretraining_parquet: data/raw/pretraining/train-00001-of-00002.parquet
  pretraining_txt: data/raw/pretraining/pretraining.txt
  pretraining_train: data/raw/pretraining/pretraining_train.pkl
  pretraining_val: data/raw/pretraining/pretraining_val.pkl
  summarization_parquet: data/raw/summary/train-00000-of-00002.parquet
  summarization_filtered_parquet: data/raw/summary/filtered.parquet
  train_full: data/raw/summary/train_full.pkl
  train_summary: data/raw/summary/train_summary.pkl
  val_full: data/raw/summary/val_full.pkl
  val_summary: data/raw/summary/val_summary.pkl
  vocab_dir: data/tokenizer/spm_model

hyperparameters:
  vocab_size: 10000
  n_embd: 512
  block_size: 128
  n_head: 8
  n_layer: 8
  dropout: 0.2
  batch_size: 32
  learning_rate: 1e-4
  max_iters: 100
  eval_iters: 50
  eval_interval: 100

mlflow_pretraining:
  server_uri: sqlite:///mlflow.db
  experiment_name: Pretraining
  run_name: 1st
  registered_model_name: Pretrained GPT

log_pretrained_model:
  model_dir: saved_models/pretrained.pth
